{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9d77f64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export HSA_FORCE_FINE_GRAIN_PCIE=1\n",
    "# export PYTORCH_ROCM_ARCH=\"gfx1100\"  # For RX 7900 XTX\n",
    "# export HIP_VISIBLE_DEVICES=0\n",
    "\n",
    "# pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/rocm5.6\n",
    "\n",
    "# rocm-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7037e513",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# train_vit_chess.py - AMD RX 7900 XTX Optimized Version\n",
    "# NOTE: AMD Optimized\n",
    "\"\"\"\n",
    "Vision‑Transformer chess move‑prediction trainer\n",
    "------------------------------------------------\n",
    "• Optimized for AMD RX 7900 XTX (24GB VRAM, RDNA3 architecture)\n",
    "• Uses ROCm optimizations and efficient memory management\n",
    "• Larger batch sizes and mixed precision training\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import math\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import functools\n",
    "import chess\n",
    "\n",
    "if torch.cuda.is_available() and \"AMD\" in torch.cuda.get_device_name():\n",
    "    # Enable AMD-specific optimizations\n",
    "    torch.backends.cuda.matmul.allow_tf32 = True\n",
    "    torch.backends.cudnn.allow_tf32 = True\n",
    "    torch.backends.cudnn.deterministic = False\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    \n",
    "    # AMD memory management\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    # Set memory fraction to use most of the 24GB\n",
    "    torch.cuda.set_per_process_memory_fraction(0.95)\n",
    "    \n",
    "    # Enable AMD's optimized attention if available\n",
    "    try:\n",
    "        torch.backends.cuda.enable_flash_sdp(True)\n",
    "        torch.backends.cuda.enable_mem_efficient_sdp(True)\n",
    "        torch.backends.cuda.enable_math_sdp(True)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "# ────────────────────────────────────────────────────────────────────────────\n",
    "# 1.  AMD RX 7900 XTX Optimized Hyper‑parameters\n",
    "# ────────────────────────────────────────────────────────────────────────────\n",
    "DATA_DIR         = \"test/tensor_full_dataset\"\n",
    "BATCH_SIZE       = 768                    # Increased for 24GB VRAM\n",
    "EPOCHS           = 15\n",
    "LR               = 5e-4                    # Slightly higher LR for larger batches\n",
    "DEVICE           = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "MAX_TOKENS       = 32\n",
    "EMBED_DIM        = 384                     # Increased model capacity\n",
    "DEPTH            = 8                      # Deeper model\n",
    "N_HEADS          = 12                     # More attention heads\n",
    "MLP_RATIO        = 4\n",
    "DROPOUT          = 0.1\n",
    "SAVE_PATH        = \"best_vit_amd.pth\"\n",
    "NUM_WORKERS      = 12  # AMD CPUs typically have more cores\n",
    "PREFETCH_FACTOR  = 4\n",
    "\n",
    "# AMD-specific optimizations\n",
    "torch.backends.cudnn.benchmark = True     # Optimize for consistent input sizes\n",
    "torch.backends.cuda.matmul.allow_tf32 = True  # Enable TF32 on supported hardware\n",
    "\n",
    "# Mixed precision training\n",
    "USE_AMP = True\n",
    "GRAD_ACCUM_STEPS = 1                      # Gradient accumulation for effective batch size 1024\n",
    "\n",
    "# ────────────────────────────────────────────────────────────────────────────\n",
    "# 2.  Optimized Dataset & loader\n",
    "# ────────────────────────────────────────────────────────────────────────────\n",
    "class PTBatchDataset(Dataset):\n",
    "    def __init__(self, pt_files, train=True, split=0.9):\n",
    "        self.pt_files = pt_files\n",
    "        self.train = train\n",
    "        self.split = split\n",
    "        self.file_indices = []\n",
    "        \n",
    "        for file_idx, f in enumerate(pt_files):\n",
    "            records = torch.load(f, map_location='cpu')  # Load to CPU first\n",
    "            split_idx = int(len(records) * split)\n",
    "            if train:\n",
    "                indices = list(range(split_idx))\n",
    "            else:\n",
    "                indices = list(range(split_idx, len(records)))\n",
    "            for record_idx in indices:\n",
    "                self.file_indices.append((file_idx, record_idx))\n",
    "        \n",
    "        # Cache for loaded files to reduce I/O\n",
    "        self._file_cache = {}\n",
    "        self._cache_size = 3  # Keep 3 files in memory\n",
    "                \n",
    "    def __len__(self):\n",
    "        return len(self.file_indices)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        file_idx, record_idx = self.file_indices[idx]\n",
    "        \n",
    "        # Use file caching to reduce I/O\n",
    "        if file_idx not in self._file_cache:\n",
    "            if len(self._file_cache) >= self._cache_size:\n",
    "                # Remove oldest file from cache\n",
    "                oldest_key = next(iter(self._file_cache))\n",
    "                del self._file_cache[oldest_key]\n",
    "            \n",
    "            self._file_cache[file_idx] = torch.load(self.pt_files[file_idx], map_location='cpu')\n",
    "        \n",
    "        rec = self._file_cache[file_idx][record_idx]\n",
    "        pos = np.asarray(rec[\"position\"], dtype=np.int16)\n",
    "        move = np.asarray(rec[\"move\"], dtype=np.uint8)\n",
    "        \n",
    "        if \"legal_mask_from\" in rec and \"legal_mask_dest\" in rec:\n",
    "            mask_from = np.asarray(rec[\"legal_mask_from\"], dtype=bool)\n",
    "            mask_dest = np.asarray(rec[\"legal_mask_dest\"], dtype=bool)\n",
    "            return pos, move, mask_from, mask_dest\n",
    "        else:\n",
    "            return pos, move\n",
    "\n",
    "def collate_fn(batch):\n",
    "    \"\"\"Optimized collate function with memory pinning.\"\"\"\n",
    "    if len(batch[0]) == 4:  # With precomputed masks\n",
    "        positions, moves, masks_from, masks_dest = zip(*batch)\n",
    "        B = len(batch)\n",
    "        \n",
    "        # Pre-allocate tensors for better memory efficiency\n",
    "        x = np.zeros((B, MAX_TOKENS, 10), dtype=np.int16)\n",
    "        pad = np.zeros((B, MAX_TOKENS), dtype=bool)\n",
    "        \n",
    "        for i, pos in enumerate(positions):\n",
    "            n = len(pos)\n",
    "            x[i, :n] = pos\n",
    "            pad[i, :n] = True\n",
    "        \n",
    "        y = np.stack(moves).astype(np.float32)\n",
    "        mask_from = np.stack(masks_from).astype(bool)\n",
    "        mask_dest = np.stack(masks_dest).astype(bool)\n",
    "        \n",
    "        return (\n",
    "            torch.as_tensor(x, dtype=torch.long),\n",
    "            torch.as_tensor(pad, dtype=torch.bool),\n",
    "            torch.as_tensor(y, dtype=torch.float32),\n",
    "            torch.as_tensor(mask_from, dtype=torch.bool),\n",
    "            torch.as_tensor(mask_dest, dtype=torch.bool),\n",
    "        )\n",
    "    else:  # Old format\n",
    "        positions, moves = zip(*batch)\n",
    "        B = len(batch)\n",
    "        x = np.zeros((B, MAX_TOKENS, 10), dtype=np.int16)\n",
    "        pad = np.zeros((B, MAX_TOKENS), dtype=bool)\n",
    "        \n",
    "        for i, pos in enumerate(positions):\n",
    "            n = len(pos)\n",
    "            x[i, :n] = pos\n",
    "            pad[i, :n] = True\n",
    "        \n",
    "        y = np.stack(moves).astype(np.float32)\n",
    "        return (\n",
    "            torch.as_tensor(x, dtype=torch.long),\n",
    "            torch.as_tensor(pad, dtype=torch.bool),\n",
    "            torch.as_tensor(y, dtype=torch.float32),\n",
    "        )\n",
    "\n",
    "# ────────────────────────────────────────────────────────────────────────────\n",
    "# 3.  Optimized Model Architecture\n",
    "# ────────────────────────────────────────────────────────────────────────────\n",
    "class TokenViT(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.embed_val = nn.Embedding(512, EMBED_DIM)\n",
    "        self.col_linear = nn.Linear(10 * EMBED_DIM, EMBED_DIM)\n",
    "        \n",
    "        # Improved initialization\n",
    "        nn.init.xavier_uniform_(self.col_linear.weight)\n",
    "        nn.init.zeros_(self.col_linear.bias)\n",
    "\n",
    "        self.cls = nn.Parameter(torch.zeros(1, 1, EMBED_DIM))\n",
    "        self.pos = nn.Parameter(torch.zeros(1, MAX_TOKENS + 1, EMBED_DIM))\n",
    "        \n",
    "        # More efficient transformer layers\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=EMBED_DIM, \n",
    "            nhead=N_HEADS,\n",
    "            dim_feedforward=EMBED_DIM * MLP_RATIO,\n",
    "            dropout=DROPOUT, \n",
    "            batch_first=True,\n",
    "            norm_first=True,  # Pre-norm for better training stability\n",
    "            activation='gelu'  # Better activation for transformers\n",
    "        )\n",
    "        self.enc = torch.compile(\n",
    "            nn.TransformerEncoder(encoder_layer, num_layers=DEPTH),\n",
    "            mode=\"max-autotune\"  # Aggressive optimization for AMD\n",
    "        )\n",
    "        self.norm = nn.LayerNorm(EMBED_DIM)\n",
    "        self.head = nn.Linear(EMBED_DIM, 128)\n",
    "        \n",
    "        # Initialize parameters\n",
    "        self._init_weights()\n",
    "\n",
    "    def _init_weights(self):\n",
    "        \"\"\"Improved weight initialization.\"\"\"\n",
    "        nn.init.normal_(self.cls, std=0.02)\n",
    "        nn.init.normal_(self.pos, std=0.02)\n",
    "        nn.init.xavier_uniform_(self.head.weight)\n",
    "        nn.init.zeros_(self.head.bias)\n",
    "\n",
    "    def forward(self, tok, padding_mask):\n",
    "        B, T, _ = tok.shape\n",
    "        e = self.embed_val(tok)\n",
    "        e = e.view(B, T, -1)\n",
    "        z = self.col_linear(e)\n",
    "\n",
    "        cls = self.cls.expand(B, -1, -1)\n",
    "        z = torch.cat([cls, z], dim=1) + self.pos[:, :T+1]\n",
    "        \n",
    "        # More efficient padding mask handling\n",
    "        pad = torch.cat([torch.zeros(B, 1, device=z.device, dtype=torch.bool), ~padding_mask], dim=1)\n",
    "        z = self.enc(z, src_key_padding_mask=pad)\n",
    "        cls_out = self.norm(z[:, 0])\n",
    "        logits = self.head(cls_out).view(B, 2, 8, 8)\n",
    "        return logits\n",
    "\n",
    "# ────────────────────────────────────────────────────────────────────────────\n",
    "# 4.  Legal‑move masking utilities (cached for performance)\n",
    "# ────────────────────────────────────────────────────────────────────────────\n",
    "@functools.lru_cache(maxsize=20000)  # Increased cache size\n",
    "def compute_single_legal_mask(pos_tuple):\n",
    "    \"\"\"Cached version for single position.\"\"\"\n",
    "    pos_tokens = list(pos_tuple)\n",
    "    mask_from = np.zeros((8, 8), dtype=bool)\n",
    "    mask_dest = np.zeros((8, 8), dtype=bool)\n",
    "    \n",
    "    board = chess.Board.empty()\n",
    "    stm = None\n",
    "    for tok in pos_tokens:\n",
    "        pid, r, f, stm_bit, wK, wQ, bK, bQ, ep, half = tok\n",
    "        if pid == 0 and r == 0 and f == 0:\n",
    "            continue\n",
    "        piece_symbol = \"PNBRQKpnbrqk\"[pid]\n",
    "        sq = chess.square(f, r)\n",
    "        board.set_piece_at(sq, chess.Piece.from_symbol(piece_symbol))\n",
    "        stm = stm_bit\n",
    "    \n",
    "    board.turn = chess.WHITE if stm == 0 else chess.BLACK\n",
    "    board.castling_rights = (\n",
    "        (chess.BB_H1 if pos_tokens[0][4] else 0) |\n",
    "        (chess.BB_A1 if pos_tokens[0][5] else 0) |\n",
    "        (chess.BB_H8 if pos_tokens[0][6] else 0) |\n",
    "        (chess.BB_A8 if pos_tokens[0][7] else 0)\n",
    "    )\n",
    "    \n",
    "    if pos_tokens[0][8] != 8:\n",
    "        board.ep_square = chess.square(pos_tokens[0][8], 5 if stm else 2)\n",
    "    \n",
    "    for mv in board.legal_moves:\n",
    "        r_from, f_from = divmod(mv.from_square, 8)\n",
    "        r_to, f_to = divmod(mv.to_square, 8)\n",
    "        mask_from[r_from, f_from] = True\n",
    "        mask_dest[r_to, f_to] = True\n",
    "    \n",
    "    return mask_from, mask_dest\n",
    "\n",
    "def legal_masks(pos_batch):\n",
    "    \"\"\"Optimized version with caching and vectorization.\"\"\"\n",
    "    B = pos_batch.shape[0]\n",
    "    mask_from = torch.zeros((B, 8, 8), dtype=torch.bool, device=DEVICE)\n",
    "    mask_dest = torch.zeros_like(mask_from)\n",
    "    \n",
    "    for b in range(B):\n",
    "        pos_tuple = tuple(tuple(tok.tolist()) for tok in pos_batch[b])\n",
    "        mf, md = compute_single_legal_mask(pos_tuple)\n",
    "        mask_from[b] = torch.from_numpy(mf)\n",
    "        mask_dest[b] = torch.from_numpy(md)\n",
    "    \n",
    "    return mask_from, mask_dest\n",
    "\n",
    "# ────────────────────────────────────────────────────────────────────────────\n",
    "# 5.  Optimized Training Loop with Mixed Precision\n",
    "# ────────────────────────────────────────────────────────────────────────────\n",
    "def run_epoch(loader, train=True):\n",
    "    model.train(mode=train)\n",
    "    total_loss, correct_top1, count = 0.0, 0, 0\n",
    "    \n",
    "    desc = \"Training\" if train else \"Validation\"\n",
    "    pbar = tqdm(loader, desc=desc, leave=False)\n",
    "    \n",
    "    # Initialize gradient scaler for mixed precision\n",
    "    scaler = torch.cuda.amp.GradScaler() if USE_AMP and train else None\n",
    "    \n",
    "    for batch_idx, batch_data in enumerate(pbar):\n",
    "        if len(batch_data) == 5:  # Precomputed masks\n",
    "            pos, pad_mask, y_true, mask_from, mask_dest = batch_data\n",
    "            mask_from = mask_from.to(DEVICE, non_blocking=True)\n",
    "            mask_dest = mask_dest.to(DEVICE, non_blocking=True)\n",
    "        else:  # Compute masks on the fly\n",
    "            pos, pad_mask, y_true = batch_data\n",
    "            mask_from, mask_dest = legal_masks(pos)\n",
    "        \n",
    "        pos = pos.to(DEVICE, non_blocking=True)\n",
    "        pad_mask = pad_mask.to(DEVICE, non_blocking=True)\n",
    "        y_true = y_true.to(DEVICE, non_blocking=True)\n",
    "\n",
    "        if train and USE_AMP:\n",
    "            with torch.cuda.amp.autocast():\n",
    "                logits = model(pos, pad_mask)\n",
    "                loss_mat = criterion(logits, y_true)\n",
    "                loss_mat[:,0] *= mask_from\n",
    "                loss_mat[:,1] *= mask_dest\n",
    "                loss = loss_mat.mean() / GRAD_ACCUM_STEPS  # Scale for gradient accumulation\n",
    "        else:\n",
    "            logits = model(pos, pad_mask)\n",
    "            loss_mat = criterion(logits, y_true)\n",
    "            loss_mat[:,0] *= mask_from\n",
    "            loss_mat[:,1] *= mask_dest\n",
    "            loss = loss_mat.mean()\n",
    "\n",
    "        if train:\n",
    "            if USE_AMP:\n",
    "                scaler.scale(loss).backward()\n",
    "                \n",
    "                # Gradient accumulation\n",
    "                if (batch_idx + 1) % GRAD_ACCUM_STEPS == 0:\n",
    "                    scaler.unscale_(opt)\n",
    "                    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "                    scaler.step(opt)\n",
    "                    scaler.update()\n",
    "                    opt.zero_grad(set_to_none=True)\n",
    "            else:\n",
    "                loss.backward()\n",
    "                if (batch_idx + 1) % GRAD_ACCUM_STEPS == 0:\n",
    "                    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "                    opt.step()\n",
    "                    opt.zero_grad(set_to_none=True)\n",
    "\n",
    "        # Metrics calculation\n",
    "        batch_loss = loss.item() * (GRAD_ACCUM_STEPS if train else 1)\n",
    "        total_loss += batch_loss * pos.size(0)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            pred_from = logits[:,0].flatten(1).argmax(1)\n",
    "            pred_dest = logits[:,1].flatten(1).argmax(1)\n",
    "            true_from = y_true[:,0].flatten(1).argmax(1)\n",
    "            true_dest = y_true[:,1].flatten(1).argmax(1)\n",
    "            batch_correct = ((pred_from == true_from) & (pred_dest == true_dest)).sum().item()\n",
    "        \n",
    "        correct_top1 += batch_correct\n",
    "        count += pos.size(0)\n",
    "        \n",
    "        pbar.set_postfix({\n",
    "            'loss': f\"{batch_loss:.4f}\",\n",
    "            'acc': f\"{100 * batch_correct / pos.size(0):.1f}%\",\n",
    "            'mem': f\"{torch.cuda.memory_allocated() / 1e9:.1f}GB\"\n",
    "        })\n",
    "\n",
    "    return total_loss / count, correct_top1 / count\n",
    "\n",
    "# ────────────────────────────────────────────────────────────────────────────\n",
    "# 6.  Main Training Loop\n",
    "# ────────────────────────────────────────────────────────────────────────────\n",
    "if __name__ == \"__main__\":\n",
    "    print(f\"Using device: {DEVICE}\")\n",
    "    print(f\"PyTorch version: {torch.__version__}\")\n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"CUDA device: {torch.cuda.get_device_name()}\")\n",
    "        print(f\"CUDA memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f}GB\")\n",
    "    \n",
    "    torch.manual_seed(42)  # For reproducibility\n",
    "    \n",
    "    # Initialize model and optimizer\n",
    "    model = TokenViT().to(DEVICE)\n",
    "    \n",
    "    # AMD-optimized optimizer settings\n",
    "    opt = torch.optim.AdamW(\n",
    "        model.parameters(), \n",
    "        lr=LR, \n",
    "        weight_decay=1e-4,\n",
    "        betas=(0.9, 0.95),  # Slightly different betas for better convergence\n",
    "        eps=1e-8\n",
    "    )\n",
    "    \n",
    "    # Learning rate scheduler\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(opt, T_max=EPOCHS, eta_min=LR*0.1)\n",
    "    \n",
    "    criterion = nn.BCEWithLogitsLoss(reduction=\"none\")\n",
    "    \n",
    "    # Data loading with optimized settings\n",
    "    pt_files = glob.glob(os.path.join(DATA_DIR, \"*.pt\"))\n",
    "    train_ds = PTBatchDataset(pt_files, train=True)\n",
    "    val_ds = PTBatchDataset(pt_files, train=False)\n",
    "    \n",
    "    # Optimized DataLoader settings for AMD GPU\n",
    "    train_ld = DataLoader(\n",
    "        train_ds, \n",
    "        batch_size=BATCH_SIZE, \n",
    "        shuffle=True,\n",
    "        num_workers=NUM_WORKERS,  # Increased for AMD\n",
    "        collate_fn=collate_fn, \n",
    "        pin_memory=True,\n",
    "        persistent_workers=True,\n",
    "        prefetch_factor=PREFETCH_FACTOR\n",
    "    )\n",
    "    val_ld = DataLoader(\n",
    "        val_ds, \n",
    "        batch_size=BATCH_SIZE, \n",
    "        shuffle=False,\n",
    "        num_workers=NUM_WORKERS, \n",
    "        collate_fn=collate_fn, \n",
    "        pin_memory=True,\n",
    "        persistent_workers=True,\n",
    "        prefetch_factor=PREFETCH_FACTOR\n",
    "    )\n",
    "\n",
    "    print(f\"Training samples: {len(train_ds)}\")\n",
    "    print(f\"Validation samples: {len(val_ds)}\")\n",
    "    print(f\"Effective batch size: {BATCH_SIZE * GRAD_ACCUM_STEPS}\")\n",
    "\n",
    "    best_val = math.inf\n",
    "    for epoch in range(1, EPOCHS+1):\n",
    "        tr_loss, tr_acc = run_epoch(train_ld, train=True)\n",
    "        vl_loss, vl_acc = run_epoch(val_ld, train=False)\n",
    "        \n",
    "        scheduler.step()  # Update learning rate\n",
    "        current_lr = scheduler.get_last_lr()[0]\n",
    "\n",
    "        print(f\"Epoch {epoch:02d} | \"\n",
    "              f\"train loss {tr_loss:.4f} acc {tr_acc*100:.1f}% || \"\n",
    "              f\"val loss {vl_loss:.4f} acc {vl_acc*100:.1f}% || \"\n",
    "              f\"lr {current_lr:.2e}\")\n",
    "\n",
    "        if vl_loss < best_val:\n",
    "            best_val = vl_loss\n",
    "            torch.save({\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': opt.state_dict(),\n",
    "                'scheduler_state_dict': scheduler.state_dict(),\n",
    "                'epoch': epoch,\n",
    "                'best_val_loss': best_val,\n",
    "                'config': {\n",
    "                    'EMBED_DIM': EMBED_DIM,\n",
    "                    'DEPTH': DEPTH,\n",
    "                    'N_HEADS': N_HEADS,\n",
    "                    'MAX_TOKENS': MAX_TOKENS\n",
    "                }\n",
    "            }, SAVE_PATH)\n",
    "            print(f\"  ✔  saved checkpoint to {SAVE_PATH}\")\n",
    "\n",
    "    print(\"Training completed!\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
