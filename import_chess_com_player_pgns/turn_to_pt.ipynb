{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6d75425c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import chess        # pip install python-chess\n",
    "\n",
    "def san_to_heatmaps(board: chess.Board, san: str, *, dtype=np.float32):\n",
    "    \"\"\"\n",
    "    Turn a SAN string (e.g. 'Nf3', 'O-O', 'e8=Q') into two 8 × 8 one‑hot planes.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    board : chess.Board\n",
    "        The position *before* the move.\n",
    "    san : str\n",
    "        Standard Algebraic Notation of the move, relative to `board`.\n",
    "    dtype : np.dtype, default np.float32\n",
    "        Data type of the returned arrays.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    origin_map, dest_map : np.ndarray, np.ndarray\n",
    "        Each is shaped (8, 8).  Squares are indexed from the White side:\n",
    "        [rank 1 is row 0, file 'a' is column 0].\n",
    "        The origin plane has a single 1 at the moved‑from square;\n",
    "        the destination plane has a 1 at the moved‑to square.\n",
    "        All other entries are 0.\n",
    "    \"\"\"\n",
    "    # ── 1. Parse SAN to a concrete Move ────────────────────────────────────\n",
    "    move = board.parse_san(san)          # raises ValueError if SAN illegal\n",
    "\n",
    "    # ── 2. Convert square indices (0–63) to rank/file coordinates ─────────\n",
    "    def square_to_rc(square: int):\n",
    "        \"\"\"a1‑square = 0 ⇒ (rank 0, file 0).\"\"\"\n",
    "        return divmod(square, 8)          # (rank, file)\n",
    "\n",
    "    r_from, c_from = square_to_rc(move.from_square)\n",
    "    r_to,   c_to   = square_to_rc(move.to_square)\n",
    "\n",
    "    # ── 3. One‑hot planes ─────────────────────────────────────────────────\n",
    "    origin = np.zeros((8, 8), dtype=dtype)\n",
    "    dest   = np.zeros((8, 8), dtype=dtype)\n",
    "    origin[r_from, c_from] = 1\n",
    "    dest[r_to,   c_to]     = 1\n",
    "\n",
    "    return origin, dest\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7a131bda",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import chess\n",
    "import os\n",
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "# def add_heatmaps_to_csv(input_dir, output_dir=None):\n",
    "#     \"\"\"\n",
    "#     Process all CSV files in the input directory, replacing the SAN move column with\n",
    "#     origin and destination heatmaps.\n",
    "    \n",
    "#     Parameters\n",
    "#     ----------\n",
    "#     input_dir : str\n",
    "#         Directory containing CSV files with FEN and MOVE (SAN) columns\n",
    "#     output_dir : str, optional\n",
    "#         Directory where processed files will be saved. If None, input files will be overwritten.\n",
    "#     \"\"\"\n",
    "#     # Create output directory if specified\n",
    "#     if output_dir is not None:\n",
    "#         os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "#     # Get list of all CSV files in input directory\n",
    "#     csv_files = list(Path(input_dir).glob(\"*.csv\"))\n",
    "#     print(f\"Found {len(csv_files)} CSV files to process\")\n",
    "    \n",
    "#     total_moves_processed = 0\n",
    "#     files_processed = 0\n",
    "#     errors = 0\n",
    "    \n",
    "#     for csv_file in csv_files:\n",
    "#         print(f\"Processing {csv_file.name}...\")\n",
    "        \n",
    "#         # Determine output path\n",
    "#         output_path = Path(output_dir) / csv_file.name if output_dir else csv_file\n",
    "        \n",
    "#         try:\n",
    "#             # Read the CSV file\n",
    "#             df = pd.read_csv(csv_file, header=0)\n",
    "            \n",
    "#             # Initialize new columns for origin and destination heatmaps\n",
    "#             origin_maps = []\n",
    "#             dest_maps = []\n",
    "            \n",
    "#             # Process each row\n",
    "#             for i, row in df.iterrows():\n",
    "#                 try:\n",
    "#                     # Extract FEN and move\n",
    "#                     fen = row['FEN']\n",
    "#                     san_move = row['MOVE']\n",
    "                    \n",
    "#                     # Create board from FEN\n",
    "#                     board = chess.Board(fen)\n",
    "                    \n",
    "#                     # Generate heatmaps\n",
    "#                     origin, dest = san_to_heatmaps(board, san_move)\n",
    "                    \n",
    "#                     # Convert heatmaps to JSON strings\n",
    "#                     origin_json = json.dumps(origin.tolist())\n",
    "#                     dest_json = json.dumps(dest.tolist())\n",
    "                    \n",
    "#                     # Add to lists\n",
    "#                     origin_maps.append(origin_json)\n",
    "#                     dest_maps.append(dest_json)\n",
    "                    \n",
    "#                     total_moves_processed += 1\n",
    "                    \n",
    "#                     # Print progress for large files\n",
    "#                     if i > 0 and i % 10000 == 0:\n",
    "#                         print(f\"  Processed {i} moves in {csv_file.name}\")\n",
    "                        \n",
    "#                 except Exception as e:\n",
    "#                     # Handle errors in individual moves\n",
    "#                     errors += 1\n",
    "#                     origin_maps.append(None)\n",
    "#                     dest_maps.append(None)\n",
    "#                     print(f\"  Error processing move at row {i}: {e}\")\n",
    "            \n",
    "#             # Create a new dataframe with FEN, ORIGIN, and DEST columns\n",
    "#             new_df = pd.DataFrame({\n",
    "#                 'FEN': df['FEN'],\n",
    "#                 'MOVE': df['MOVE'],\n",
    "#                 'ORIGIN': origin_maps,\n",
    "#                 'DEST': dest_maps\n",
    "#             })\n",
    "            \n",
    "#             # Save the updated dataframe\n",
    "#             new_df.to_csv(output_path, index=False)\n",
    "            \n",
    "#             files_processed += 1\n",
    "            \n",
    "#         except Exception as e:\n",
    "#             print(f\"Error processing file {csv_file.name}: {e}\")\n",
    "    \n",
    "#     print(f\"\\nProcessing complete!\")\n",
    "#     print(f\"Files processed: {files_processed}/{len(csv_files)}\")\n",
    "#     print(f\"Total moves processed: {total_moves_processed}\")\n",
    "#     print(f\"Errors encountered: {errors}\")\n",
    "\n",
    "# ...existing code...\n",
    "def add_heatmaps_to_csv(input_dir, output_dir=None):\n",
    "    \"\"\"\n",
    "    Process all CSV files in the input directory, replacing the SAN move column with\n",
    "    origin and destination heatmaps. Rows that fail to parse are DROPPED.\n",
    "    \"\"\"\n",
    "    if output_dir is not None:\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    csv_files = list(Path(input_dir).glob(\"*.csv\"))\n",
    "    print(f\"Found {len(csv_files)} CSV files to process\")\n",
    "\n",
    "    total_moves_processed = 0\n",
    "    files_processed = 0\n",
    "    errors = 0\n",
    "\n",
    "    for csv_file in csv_files:\n",
    "        print(f\"Processing {csv_file.name}...\")\n",
    "        output_path = Path(output_dir) / csv_file.name if output_dir else csv_file\n",
    "\n",
    "        try:\n",
    "            df = pd.read_csv(csv_file, header=0)\n",
    "\n",
    "            kept_fens = []\n",
    "            kept_moves = []\n",
    "            origin_maps = []\n",
    "            dest_maps = []\n",
    "\n",
    "            for i, row in df.iterrows():\n",
    "                try:\n",
    "                    fen = row['FEN']\n",
    "                    san_move = row['MOVE']\n",
    "\n",
    "                    # strip crazyhouse pocket if present (e.g. ...[Nn])\n",
    "                    first_field = fen.split()[0]\n",
    "                    if '[' in first_field:\n",
    "                        parts = fen.split()\n",
    "                        parts[0] = first_field.split('[', 1)[0]\n",
    "                        fen = ' '.join(parts)\n",
    "\n",
    "                    board = chess.Board(fen)\n",
    "                    origin, dest = san_to_heatmaps(board, san_move)\n",
    "\n",
    "                    origin_json = json.dumps(origin.tolist())\n",
    "                    dest_json = json.dumps(dest.tolist())\n",
    "\n",
    "                    kept_fens.append(fen)\n",
    "                    kept_moves.append(san_move)\n",
    "                    origin_maps.append(origin_json)\n",
    "                    dest_maps.append(dest_json)\n",
    "\n",
    "                    total_moves_processed += 1\n",
    "                    if total_moves_processed % 10000 == 0:\n",
    "                        print(f\"  Processed {total_moves_processed} total moves so far...\")\n",
    "                except Exception as e:\n",
    "                    errors += 1\n",
    "                    # drop the row entirely (do not append placeholders)\n",
    "                    if errors <= 10:\n",
    "                        print(f\"  Dropped row {i} (error: {e})\")\n",
    "                    continue\n",
    "\n",
    "            new_df = pd.DataFrame({\n",
    "                'FEN': kept_fens,\n",
    "                'MOVE': kept_moves,\n",
    "                'ORIGIN': origin_maps,\n",
    "                'DEST': dest_maps\n",
    "            })\n",
    "\n",
    "            new_df.to_csv(output_path, index=False)\n",
    "            print(f\"  Wrote {len(new_df)} clean rows (dropped {errors} so far cumulative).\")\n",
    "            files_processed += 1\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing file {csv_file.name}: {e}\")\n",
    "\n",
    "    print(f\"\\nProcessing complete!\")\n",
    "    print(f\"Files processed: {files_processed}/{len(csv_files)}\")\n",
    "    print(f\"Total kept moves: {total_moves_processed}\")\n",
    "    print(f\"Total rows dropped: {errors}\")\n",
    "# ...existing code..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3afb1c25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 18 CSV files to process\n",
      "Processing all_GMWSO_black_2020-08-14_to_2025-08-13_fen_moves.csv...\n",
      "  Dropped row 348 (error: invalid half-move clock in fen: 'rn1qkbnr/pbpppppp/1p6/8/2B1P3/2N5/PPPP1PPP/R1BQK1NR b KQkq - 3+3 3 3')\n",
      "  Dropped row 349 (error: invalid half-move clock in fen: 'rn1qkbnr/pbpp1ppp/1p2p3/8/2B1P3/2N2N2/PPPP1PPP/R1BQK2R b KQkq - 3+3 1 4')\n",
      "  Dropped row 350 (error: invalid half-move clock in fen: 'rn1qkb1r/pbpp1ppp/1p2pn2/8/2B1P3/2N2N2/PPPP1PPP/R1BQ1RK1 b kq - 3+3 3 5')\n",
      "  Dropped row 351 (error: invalid half-move clock in fen: 'rn1qkb1r/pbpp1ppp/1p2p3/8/2B1N3/5N2/PPPP1PPP/R1BQ1RK1 b kq - 3+3 0 6')\n",
      "  Dropped row 352 (error: invalid half-move clock in fen: 'rn1qkb1r/p1pp1ppp/1p2p3/8/2B1b3/5N2/PPPP1PPP/R1BQR1K1 b kq - 3+3 1 7')\n",
      "  Dropped row 353 (error: invalid half-move clock in fen: 'rn1qkb1r/p1pp1ppp/1p2p1b1/8/2BP4/5N2/PPP2PPP/R1BQR1K1 b kq - 3+3 0 8')\n",
      "  Dropped row 354 (error: invalid half-move clock in fen: 'rn1qk2r/p1ppbppp/1p2p1b1/3P4/2B5/5N2/PPP2PPP/R1BQR1K1 b kq - 3+3 0 9')\n",
      "  Dropped row 355 (error: invalid half-move clock in fen: 'rn1q1rk1/p1ppbppp/1p2p1b1/3PN3/2B5/8/PPP2PPP/R1BQR1K1 b - - 3+3 2 10')\n",
      "  Dropped row 356 (error: invalid half-move clock in fen: 'rn1q1rk1/p1pp1ppp/1p2Pbb1/4N3/2B5/8/PPP2PPP/R1BQR1K1 b - - 3+3 0 11')\n",
      "  Dropped row 357 (error: invalid half-move clock in fen: 'rn1q1rk1/p1p2ppp/1p2pbb1/4N3/2B2B2/8/PPP2PPP/R2QR1K1 b - - 3+3 1 12')\n",
      "  Processed 10000 total moves so far...\n",
      "  Processed 20000 total moves so far...\n",
      "  Processed 30000 total moves so far...\n",
      "  Processed 40000 total moves so far...\n",
      "  Processed 50000 total moves so far...\n",
      "  Processed 60000 total moves so far...\n",
      "  Processed 70000 total moves so far...\n",
      "  Processed 80000 total moves so far...\n",
      "  Processed 90000 total moves so far...\n",
      "  Wrote 90076 clean rows (dropped 85 so far cumulative).\n",
      "Processing all_MagnusCarlsen_black_2020-08-14_to_2025-08-13_fen_moves.csv...\n",
      "  Processed 100000 total moves so far...\n",
      "  Processed 110000 total moves so far...\n",
      "  Processed 120000 total moves so far...\n",
      "  Processed 130000 total moves so far...\n",
      "  Processed 140000 total moves so far...\n",
      "  Processed 150000 total moves so far...\n",
      "  Processed 160000 total moves so far...\n",
      "  Wrote 78240 clean rows (dropped 85 so far cumulative).\n",
      "Processing all_LevonAronian_black_2020-08-14_to_2025-08-13_fen_moves.csv...\n",
      "  Processed 170000 total moves so far...\n",
      "  Processed 180000 total moves so far...\n",
      "  Wrote 16457 clean rows (dropped 85 so far cumulative).\n",
      "Processing all_Chefshouse_black_2020-08-14_to_2025-08-13_fen_moves.csv...\n",
      "  Processed 190000 total moves so far...\n",
      "  Wrote 5687 clean rows (dropped 85 so far cumulative).\n",
      "Processing all_FabianoCaruana_black_2020-08-14_to_2025-08-13_fen_moves.csv...\n",
      "  Processed 200000 total moves so far...\n",
      "  Wrote 12759 clean rows (dropped 85 so far cumulative).\n",
      "Processing all_GukeshDommaraju_black_2020-08-14_to_2025-08-13_fen_moves.csv...\n",
      "  Processed 210000 total moves so far...\n",
      "  Processed 220000 total moves so far...\n",
      "  Wrote 25302 clean rows (dropped 88 so far cumulative).\n",
      "Processing all_LevonAronian_white_2020-08-14_to_2025-08-13_fen_moves.csv...\n",
      "  Processed 230000 total moves so far...\n",
      "  Processed 240000 total moves so far...\n",
      "  Wrote 16106 clean rows (dropped 88 so far cumulative).\n",
      "Processing all_Chefshouse_white_2020-08-14_to_2025-08-13_fen_moves.csv...\n",
      "  Processed 250000 total moves so far...\n",
      "  Wrote 5525 clean rows (dropped 88 so far cumulative).\n",
      "Processing all_MagnusCarlsen_white_2020-08-14_to_2025-08-13_fen_moves.csv...\n",
      "  Processed 260000 total moves so far...\n",
      "  Processed 270000 total moves so far...\n",
      "  Processed 280000 total moves so far...\n",
      "  Processed 290000 total moves so far...\n",
      "  Processed 300000 total moves so far...\n",
      "  Processed 310000 total moves so far...\n",
      "  Processed 320000 total moves so far...\n",
      "  Wrote 76965 clean rows (dropped 112 so far cumulative).\n",
      "Processing all_GMWSO_white_2020-08-14_to_2025-08-13_fen_moves.csv...\n",
      "  Processed 330000 total moves so far...\n",
      "  Processed 340000 total moves so far...\n",
      "  Processed 350000 total moves so far...\n",
      "  Processed 360000 total moves so far...\n",
      "  Processed 370000 total moves so far...\n",
      "  Processed 380000 total moves so far...\n",
      "  Processed 390000 total moves so far...\n",
      "  Processed 400000 total moves so far...\n",
      "  Processed 410000 total moves so far...\n",
      "  Wrote 89739 clean rows (dropped 287 so far cumulative).\n",
      "Processing all_GukeshDommaraju_white_2020-08-14_to_2025-08-13_fen_moves.csv...\n",
      "  Processed 420000 total moves so far...\n",
      "  Processed 430000 total moves so far...\n",
      "  Processed 440000 total moves so far...\n",
      "  Wrote 25957 clean rows (dropped 295 so far cumulative).\n",
      "Processing all_FabianoCaruana_white_2020-08-14_to_2025-08-13_fen_moves.csv...\n",
      "  Processed 450000 total moves so far...\n",
      "  Wrote 13850 clean rows (dropped 295 so far cumulative).\n",
      "Processing all_AnishGiri_white_2020-08-14_to_2025-08-13_fen_moves.csv...\n",
      "  Processed 460000 total moves so far...\n",
      "  Processed 470000 total moves so far...\n",
      "  Wrote 21613 clean rows (dropped 295 so far cumulative).\n",
      "Processing all_LyonBeast_white_2020-08-14_to_2025-08-13_fen_moves.csv...\n",
      "  Processed 480000 total moves so far...\n",
      "  Wrote 10898 clean rows (dropped 331 so far cumulative).\n",
      "Processing all_rpragchess_black_2020-08-14_to_2025-08-13_fen_moves.csv...\n",
      "  Processed 490000 total moves so far...\n",
      "  Processed 500000 total moves so far...\n",
      "  Processed 510000 total moves so far...\n",
      "  Processed 520000 total moves so far...\n",
      "  Processed 530000 total moves so far...\n",
      "  Processed 540000 total moves so far...\n",
      "  Processed 550000 total moves so far...\n",
      "  Processed 560000 total moves so far...\n",
      "  Processed 570000 total moves so far...\n",
      "  Processed 580000 total moves so far...\n",
      "  Wrote 94797 clean rows (dropped 376 so far cumulative).\n",
      "Processing all_LyonBeast_black_2020-08-14_to_2025-08-13_fen_moves.csv...\n",
      "  Processed 590000 total moves so far...\n",
      "  Wrote 10530 clean rows (dropped 398 so far cumulative).\n",
      "Processing all_rpragchess_white_2020-08-14_to_2025-08-13_fen_moves.csv...\n",
      "  Processed 600000 total moves so far...\n",
      "  Processed 610000 total moves so far...\n",
      "  Processed 620000 total moves so far...\n",
      "  Processed 630000 total moves so far...\n",
      "  Processed 640000 total moves so far...\n",
      "  Processed 650000 total moves so far...\n",
      "  Processed 660000 total moves so far...\n",
      "  Processed 670000 total moves so far...\n",
      "  Processed 680000 total moves so far...\n",
      "  Processed 690000 total moves so far...\n",
      "  Wrote 95962 clean rows (dropped 439 so far cumulative).\n",
      "Processing all_AnishGiri_black_2020-08-14_to_2025-08-13_fen_moves.csv...\n",
      "  Processed 700000 total moves so far...\n",
      "  Processed 710000 total moves so far...\n",
      "  Wrote 22226 clean rows (dropped 439 so far cumulative).\n",
      "\n",
      "Processing complete!\n",
      "Files processed: 18/18\n",
      "Total kept moves: 712689\n",
      "Total rows dropped: 439\n"
     ]
    }
   ],
   "source": [
    "add_heatmaps_to_csv(\"fen_moves_output\", \"fen_moves_output_heatmaps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "12cde18c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 9,  7,  0,  1,  1,  1,  1,  1,  8,  0],\n",
       "       [ 7,  7,  1,  1,  1,  1,  1,  1,  8,  0],\n",
       "       [ 8,  7,  2,  1,  1,  1,  1,  1,  8,  0],\n",
       "       [10,  7,  3,  1,  1,  1,  1,  1,  8,  0],\n",
       "       [11,  7,  4,  1,  1,  1,  1,  1,  8,  0],\n",
       "       [ 8,  7,  5,  1,  1,  1,  1,  1,  8,  0],\n",
       "       [ 7,  7,  6,  1,  1,  1,  1,  1,  8,  0],\n",
       "       [ 9,  7,  7,  1,  1,  1,  1,  1,  8,  0],\n",
       "       [ 6,  6,  0,  1,  1,  1,  1,  1,  8,  0],\n",
       "       [ 6,  6,  1,  1,  1,  1,  1,  1,  8,  0],\n",
       "       [ 6,  6,  3,  1,  1,  1,  1,  1,  8,  0],\n",
       "       [ 6,  6,  4,  1,  1,  1,  1,  1,  8,  0],\n",
       "       [ 6,  6,  5,  1,  1,  1,  1,  1,  8,  0],\n",
       "       [ 6,  6,  7,  1,  1,  1,  1,  1,  8,  0],\n",
       "       [ 6,  5,  6,  1,  1,  1,  1,  1,  8,  0],\n",
       "       [ 6,  4,  2,  1,  1,  1,  1,  1,  8,  0],\n",
       "       [ 0,  3,  2,  1,  1,  1,  1,  1,  8,  0],\n",
       "       [ 1,  2,  2,  1,  1,  1,  1,  1,  8,  0],\n",
       "       [ 0,  2,  6,  1,  1,  1,  1,  1,  8,  0],\n",
       "       [ 0,  1,  0,  1,  1,  1,  1,  1,  8,  0],\n",
       "       [ 0,  1,  1,  1,  1,  1,  1,  1,  8,  0],\n",
       "       [ 0,  1,  3,  1,  1,  1,  1,  1,  8,  0],\n",
       "       [ 0,  1,  4,  1,  1,  1,  1,  1,  8,  0],\n",
       "       [ 0,  1,  5,  1,  1,  1,  1,  1,  8,  0],\n",
       "       [ 0,  1,  7,  1,  1,  1,  1,  1,  8,  0],\n",
       "       [ 3,  0,  0,  1,  1,  1,  1,  1,  8,  0],\n",
       "       [ 2,  0,  2,  1,  1,  1,  1,  1,  8,  0],\n",
       "       [ 4,  0,  3,  1,  1,  1,  1,  1,  8,  0],\n",
       "       [ 5,  0,  4,  1,  1,  1,  1,  1,  8,  0],\n",
       "       [ 2,  0,  5,  1,  1,  1,  1,  1,  8,  0],\n",
       "       [ 1,  0,  6,  1,  1,  1,  1,  1,  8,  0],\n",
       "       [ 3,  0,  7,  1,  1,  1,  1,  1,  8,  0]], dtype=int16)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import chess\n",
    "import chess.pgn\n",
    "import os\n",
    "import csv\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def fen_to_piece_tokens(fen: str, *, tensor: bool = False, max_halfmove: int = 100):\n",
    "    \"\"\"\n",
    "    Convert a FEN position to the *piece‑token patching* encoding.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    fen : str\n",
    "        Standard Forsyth–Edwards Notation string.\n",
    "    tensor : bool, default False\n",
    "        If True, return a `torch.LongTensor`; otherwise a `np.ndarray`.\n",
    "    max_halfmove : int, default 100\n",
    "        Clamp the half‑move clock to this upper bound (helps normalisation).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    tokens : np.ndarray | torch.LongTensor\n",
    "        Shape = (N_pieces, 10) with columns:\n",
    "\n",
    "        0  piece_id              0–5 = white P,N,B,R,Q,K ; 6–11 = black P…K  \n",
    "        1  rank                  0–7  (0 = rank 1 from White’s view)  \n",
    "        2  file                  0–7  (0 = file ‘a’)  \n",
    "        3  side_to_move          0 = white, 1 = black  \n",
    "        4  white_castle_K        0/1  \n",
    "        5  white_castle_Q        0/1  \n",
    "        6  black_castle_K        0/1  \n",
    "        7  black_castle_Q        0/1  \n",
    "        8  en_passant_file       0–7 if ep target exists else **8**  \n",
    "        9  halfmove_clock        0–`max_halfmove`\n",
    "    \"\"\"\n",
    "    # --- split FEN into its 6 fields ---------------------------------------\n",
    "    board_fen, stm, castles, ep_sq, half_clk, _ = fen.split()\n",
    "\n",
    "    piece_to_id = {c: i for i, c in enumerate(\"PNBRQKpnbrqk\")}\n",
    "\n",
    "    # --- global‑state scalars (replicated into every token) -----------------\n",
    "    stm_bit = 0 if stm == \"w\" else 1\n",
    "    castle_bits = (\n",
    "        int(\"K\" in castles),  # white K‑side\n",
    "        int(\"Q\" in castles),  # white Q‑side\n",
    "        int(\"k\" in castles),  # black K‑side\n",
    "        int(\"q\" in castles),  # black Q‑side\n",
    "    )\n",
    "    ep_file = ord(ep_sq[0]) - ord(\"a\") if ep_sq != \"-\" else 8\n",
    "    half_clk = min(int(half_clk), max_halfmove)\n",
    "\n",
    "    # --- walk through the 8×8 board ----------------------------------------\n",
    "    tokens = []\n",
    "    rank_idx = 7                              # FEN starts from rank 8\n",
    "    for row in board_fen.split(\"/\"):\n",
    "        file_idx = 0\n",
    "        for ch in row:\n",
    "            if ch.isdigit():                  # empty squares\n",
    "                file_idx += int(ch)\n",
    "            else:                             # occupied square → token\n",
    "                tokens.append([\n",
    "                    piece_to_id[ch],\n",
    "                    rank_idx,\n",
    "                    file_idx,\n",
    "                    stm_bit,\n",
    "                    *castle_bits,\n",
    "                    ep_file,\n",
    "                    half_clk,\n",
    "                ])\n",
    "                file_idx += 1\n",
    "        rank_idx -= 1\n",
    "\n",
    "    arr = np.asarray(tokens, dtype=np.int16)\n",
    "    if tensor:\n",
    "        import torch\n",
    "        return torch.as_tensor(arr, dtype=torch.long)\n",
    "    return arr\n",
    "\n",
    "\n",
    "fen_to_piece_tokens('rnbqkbnr/pp1ppp1p/6p1/2p5/2P5/2N3P1/PP1PPP1P/R1BQKBNR b KQkq - 0 3', tensor=False, max_halfmove=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "410f77b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_legal_masks_for_position(pos_tokens):\n",
    "    \"\"\"\n",
    "    Compute legal masks for a single position.\n",
    "    Returns: (mask_from, mask_dest) as numpy arrays (8,8) bool\n",
    "    \"\"\"\n",
    "    mask_from = np.zeros((8, 8), dtype=bool)\n",
    "    mask_dest = np.zeros((8, 8), dtype=bool)\n",
    "    \n",
    "    # Rebuild board from tokens\n",
    "    board = chess.Board.empty()\n",
    "    stm = None\n",
    "    for tok in pos_tokens:\n",
    "        pid, r, f, stm_bit, wK, wQ, bK, bQ, ep, half = tok\n",
    "        if pid == 0 and r == 0 and f == 0:  # padding\n",
    "            continue\n",
    "        piece_symbol = \"PNBRQKpnbrqk\"[pid]\n",
    "        sq = chess.square(f, r)\n",
    "        board.set_piece_at(sq, chess.Piece.from_symbol(piece_symbol))\n",
    "        stm = stm_bit\n",
    "    \n",
    "    board.turn = chess.WHITE if stm == 0 else chess.BLACK\n",
    "    board.castling_rights = (\n",
    "        (chess.BB_H1 if pos_tokens[0][4] else 0) |  # wK\n",
    "        (chess.BB_A1 if pos_tokens[0][5] else 0) |  # wQ\n",
    "        (chess.BB_H8 if pos_tokens[0][6] else 0) |  # bK\n",
    "        (chess.BB_A8 if pos_tokens[0][7] else 0)    # bQ\n",
    "    )\n",
    "    \n",
    "    # En passant\n",
    "    if pos_tokens[0][8] != 8:\n",
    "        board.ep_square = chess.square(pos_tokens[0][8], 5 if stm else 2)\n",
    "    \n",
    "    # Collect legal moves\n",
    "    for mv in board.legal_moves:\n",
    "        r_from, f_from = divmod(mv.from_square, 8)\n",
    "        r_to, f_to = divmod(mv.to_square, 8)\n",
    "        mask_from[r_from, f_from] = True\n",
    "        mask_dest[r_to, f_to] = True\n",
    "    \n",
    "    return mask_from, mask_dest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b31d6b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import os\n",
    "import gc\n",
    "import json\n",
    "import time\n",
    "\n",
    "def preprocess_csv_to_tensors_robust(input_dir, output_dir, batch_size=50000, resume=True, max_positions=None):\n",
    "    \"\"\"\n",
    "    Process all CSV files with FEN+move pairs into multiple batch files\n",
    "    with more robust file handling to prevent corruption\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    input_dir : str\n",
    "        Directory containing CSV files with FEN and MOVE columns\n",
    "    output_dir : str\n",
    "        Directory where the output tensor files will be saved\n",
    "    batch_size : int\n",
    "        Number of positions to process before saving to disk\n",
    "    resume : bool\n",
    "        Whether to resume processing from last saved batch\n",
    "    max_positions : int or None\n",
    "        Maximum number of positions to process (None for unlimited)\n",
    "    \"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Path for metadata and progress tracking\n",
    "    metadata_file = Path(output_dir) / \"dataset_metadata.json\"\n",
    "    progress_file = Path(output_dir) / \"processing_progress.json\"\n",
    "    index_file = Path(output_dir) / \"batch_index.json\"\n",
    "    \n",
    "    # Initialize or load progress tracking\n",
    "    progress = {\n",
    "        'total_positions': 0,\n",
    "        'files_processed': [],\n",
    "        'current_file': None,\n",
    "        'current_file_position': 0,\n",
    "        'last_batch_num': -1\n",
    "    }\n",
    "    \n",
    "    # Track batches\n",
    "    batch_index = {\n",
    "        'batch_files': [],\n",
    "        'positions_per_batch': [],\n",
    "        'total_positions': 0\n",
    "    }\n",
    "    \n",
    "    # Check if we can resume from previous processing\n",
    "    if resume and progress_file.exists():\n",
    "        try:\n",
    "            with open(progress_file, 'r') as f:\n",
    "                progress = json.load(f)\n",
    "            print(f\"Resuming from previous processing. Already processed {progress['total_positions']} positions.\")\n",
    "            \n",
    "            if index_file.exists():\n",
    "                with open(index_file, 'r') as f:\n",
    "                    batch_index = json.load(f)\n",
    "                print(f\"Found {len(batch_index['batch_files'])} existing batch files\")\n",
    "        except Exception as e:\n",
    "            print(f\"Could not load progress file: {e}. Starting from scratch.\")\n",
    "            progress = {\n",
    "                'total_positions': 0,\n",
    "                'files_processed': [],\n",
    "                'current_file': None,\n",
    "                'current_file_position': 0,\n",
    "                'last_batch_num': -1\n",
    "            }\n",
    "            batch_index = {\n",
    "                'batch_files': [],\n",
    "                'positions_per_batch': [],\n",
    "                'total_positions': 0\n",
    "            }\n",
    "    \n",
    "    # Get the list of CSV files\n",
    "    all_csv_files = sorted(list(Path(input_dir).glob(\"*.csv\")))\n",
    "    \n",
    "    # Filter out already processed files\n",
    "    csv_files = [f for f in all_csv_files if str(f) not in progress['files_processed']]\n",
    "    \n",
    "    print(f\"Found {len(all_csv_files)} total CSV files, {len(csv_files)} remaining to process\")\n",
    "    \n",
    "    if len(csv_files) == 0:\n",
    "        print(\"All files already processed. Nothing to do.\")\n",
    "        return progress['total_positions']\n",
    "    \n",
    "    # Initialize batch tracking\n",
    "    batch_data = []\n",
    "    total_positions = progress['total_positions']\n",
    "    batch_num = progress['last_batch_num'] + 1\n",
    "    \n",
    "    # Function to save a batch as a separate file\n",
    "    def save_batch(batch_data):\n",
    "        nonlocal batch_num\n",
    "        \n",
    "        if not batch_data:\n",
    "            return\n",
    "            \n",
    "        # Create a unique filename for this batch\n",
    "        batch_file = f\"batch_{batch_num:06d}.pt\"\n",
    "        output_path = Path(output_dir) / batch_file\n",
    "        temp_path = output_path.with_suffix('.tmp')\n",
    "        \n",
    "        # Save to temp file first for safety\n",
    "        try:\n",
    "            torch.save(batch_data, temp_path)\n",
    "            \n",
    "            # Make sure the file is fully written (properly manage file handle)\n",
    "            with open(temp_path, 'rb') as f:\n",
    "                os.fsync(f.fileno())\n",
    "            \n",
    "            # Rename the temp file to final filename (atomic operation)\n",
    "            temp_path.rename(output_path)\n",
    "            \n",
    "            # Update the batch index\n",
    "            batch_index['batch_files'].append(batch_file)\n",
    "            batch_index['positions_per_batch'].append(len(batch_data))\n",
    "            batch_index['total_positions'] += len(batch_data)\n",
    "            \n",
    "            # Move to next batch\n",
    "            batch_num += 1\n",
    "            progress['last_batch_num'] = batch_num - 1\n",
    "            \n",
    "            print(f\"  Saved batch {batch_num-1} with {len(batch_data)} positions to {batch_file}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  Error saving batch: {e}\")\n",
    "            if temp_path.exists():\n",
    "                try:\n",
    "                    temp_path.unlink()\n",
    "                except:\n",
    "                    pass\n",
    "            \n",
    "        # Clear batch data and collect garbage\n",
    "        batch_data.clear()\n",
    "        gc.collect()\n",
    "    \n",
    "    # Function to save progress\n",
    "    def save_progress():\n",
    "        with open(progress_file, 'w') as f:\n",
    "            json.dump(progress, f, indent=2)\n",
    "        with open(index_file, 'w') as f:\n",
    "            json.dump(batch_index, f, indent=2)\n",
    "    \n",
    "    # Process each CSV file\n",
    "    try:\n",
    "        for csv_file in csv_files:\n",
    "            # Skip if already fully processed\n",
    "            if str(csv_file) in progress['files_processed']:\n",
    "                continue\n",
    "                \n",
    "            print(f\"Processing {csv_file.name}...\")\n",
    "            progress['current_file'] = str(csv_file)\n",
    "            \n",
    "            try:\n",
    "                # Read the CSV file\n",
    "                df = pd.read_csv(csv_file)\n",
    "                \n",
    "                # If resuming from middle of a file\n",
    "                start_idx = progress['current_file_position'] if csv_file.name == Path(progress['current_file']).name else 0\n",
    "                \n",
    "                # Process each FEN string\n",
    "                for idx, row in df.iloc[start_idx:].iterrows():\n",
    "                    # Check if we've reached the maximum positions\n",
    "                    if max_positions and total_positions >= max_positions:\n",
    "                        print(f\"Reached maximum of {max_positions} positions. Stopping.\")\n",
    "                        # Save any remaining data\n",
    "                        if batch_data:\n",
    "                            save_batch(batch_data)\n",
    "                        save_progress()\n",
    "                        return total_positions\n",
    "                        \n",
    "                    fen = row['FEN']\n",
    "                    origin_data = json.loads(row['ORIGIN'])\n",
    "                    dest_data = json.loads(row['DEST'])\n",
    "                    origin_tensor = torch.as_tensor(origin_data, dtype=torch.float32)\n",
    "                    dest_tensor = torch.as_tensor(dest_data, dtype=torch.float32)\n",
    "                    move = torch.stack([origin_tensor, dest_tensor], dim=0)\n",
    "                    \n",
    "                    # Convert FEN to tensor\n",
    "                    position_tensor = fen_to_piece_tokens(fen, tensor=True)\n",
    "                    \n",
    "                    mask_from, mask_dest = compute_legal_masks_for_position(position_tensor)\n",
    "        \n",
    "                    \n",
    "                    # Store the tensor along with the move\n",
    "                    batch_data.append({\n",
    "                        'position': position_tensor,\n",
    "                        'move': move,\n",
    "                        'source_file': csv_file.stem,\n",
    "                        'legal_mask_from': mask_from.astype(np.uint8),\n",
    "                        'legal_mask_dest': mask_dest.astype(np.uint8),\n",
    "                    })\n",
    "                    \n",
    "                    total_positions += 1\n",
    "                    progress['total_positions'] = total_positions\n",
    "                    progress['current_file_position'] = idx + 1\n",
    "                    \n",
    "                    # Print progress periodically\n",
    "                    if total_positions % 10000 == 0:\n",
    "                        print(f\"  Processed {total_positions} positions so far...\")\n",
    "                    \n",
    "                    # Save batch when reaching batch_size\n",
    "                    if len(batch_data) >= batch_size:\n",
    "                        save_batch(batch_data)\n",
    "                        # Update and save progress\n",
    "                        save_progress()\n",
    "                \n",
    "                # Mark file as fully processed\n",
    "                progress['files_processed'].append(str(csv_file))\n",
    "                progress['current_file_position'] = 0\n",
    "                save_progress()\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"  Error processing {csv_file.name}: {e}\")\n",
    "                # Save progress before exiting this file\n",
    "                save_progress()\n",
    "                \n",
    "                # Save any partial batch data\n",
    "                if batch_data:\n",
    "                    save_batch(batch_data)\n",
    "        \n",
    "        # Save any remaining data in the final batch\n",
    "        if batch_data:\n",
    "            print(f\"  Saving final batch of {len(batch_data)} positions...\")\n",
    "            save_batch(batch_data)\n",
    "        \n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\nProcess interrupted by user.\")\n",
    "        # Save any partial batch data\n",
    "        if batch_data:\n",
    "            save_batch(batch_data)\n",
    "        # Save progress\n",
    "        save_progress()\n",
    "        return total_positions\n",
    "    \n",
    "    print(f\"\\nProcessing complete. Processed a total of {total_positions} positions.\")\n",
    "    \n",
    "    # Save the final metadata file\n",
    "    metadata = {\n",
    "        'total_positions': total_positions,\n",
    "        'source_files': [f.stem for f in all_csv_files],\n",
    "        'creation_date': pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "        'batch_processing': True,\n",
    "        'batch_size': batch_size,\n",
    "        'total_batches': len(batch_index['batch_files'])\n",
    "    }\n",
    "    \n",
    "    with open(metadata_file, 'w') as f:\n",
    "        json.dump(metadata, f, indent=2)\n",
    "    \n",
    "    # Keep the progress file for information\n",
    "    return total_positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbe2efd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 18 total CSV files, 18 remaining to process\n",
      "Processing all_AnishGiri_black_2020-08-14_to_2025-08-13_fen_moves.csv...\n",
      "  Processed 10000 positions so far...\n",
      "  Processed 20000 positions so far...\n",
      "Processing all_AnishGiri_white_2020-08-14_to_2025-08-13_fen_moves.csv...\n",
      "  Processed 30000 positions so far...\n"
     ]
    }
   ],
   "source": [
    "preprocess_csv_to_tensors_robust(\"fen_moves_output_heatmaps\", \"fen_moves_output_tensors\", batch_size=3000000, resume=True, max_positions=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "523dbb89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 6 source batch files...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Regrouping: 100%|██████████| 6/6 [01:47<00:00, 17.96s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Grouping complete. Created:\n",
      "  all_AnishGiri.pt  43839 records  (262.1 MB)\n",
      "  all_Chefshouse.pt  11212 records  (68.4 MB)\n",
      "  all_FabianoCaruana.pt  26609 records  (161.0 MB)\n",
      "  all_GukeshDommaraju.pt  25965 records  (156.0 MB)\n",
      "  all_LevonAronian.pt  32563 records  (194.1 MB)\n",
      "  all_LyonBeast.pt  10934 records  (65.8 MB)\n",
      "  all_MagnusCarlsen.pt  128777 records  (765.8 MB)\n",
      "  all_WesleySo.pt  39377 records  (118.6 MB)\n",
      "  all_rpragchess.pt  62109 records  (221.5 MB)\n",
      "\n",
      "Total records regrouped: 381385\n"
     ]
    }
   ],
   "source": [
    "# ...existing code...\n",
    "\n",
    "import os, glob\n",
    "import torch\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "\n",
    "def regroup_batches_by_source_prefix(\n",
    "    src_dir: str,\n",
    "    dst_dir: str,\n",
    "    prefix_parts: int = 2,\n",
    "    flush_size: int = 50_000\n",
    "):\n",
    "    \"\"\"\n",
    "    Group records from batch_*.pt files by the first `prefix_parts` underscore-\n",
    "    separated tokens of `record['source_file']` and write one .pt per group.\n",
    "\n",
    "    Example:\n",
    "      source_file = \"all_FabianoCaruana_black_2020-08-14_to_2025-08-13_fen_moves\"\n",
    "      prefix_parts = 2  -> key = \"all_FabianoCaruana\" -> all_FabianoCaruana.pt\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    src_dir : directory containing original batch_*.pt files\n",
    "    dst_dir : output directory for grouped .pt files\n",
    "    prefix_parts : number of leading underscore tokens to form the group key\n",
    "    flush_size : flush buffer to disk after this many accumulated records per key\n",
    "    \"\"\"\n",
    "    os.makedirs(dst_dir, exist_ok=True)\n",
    "\n",
    "    batch_files = sorted(glob.glob(os.path.join(src_dir, \"batch_*.pt\")))\n",
    "    if not batch_files:\n",
    "        print(\"No batch_*.pt files found.\")\n",
    "        return\n",
    "\n",
    "    buffers = defaultdict(list)\n",
    "    counts = defaultdict(int)\n",
    "\n",
    "    def flush_key(key):\n",
    "        \"\"\"Append buffer for key to its .pt file and clear buffer.\"\"\"\n",
    "        buf = buffers[key]\n",
    "        if not buf:\n",
    "            return\n",
    "        out_path = os.path.join(dst_dir, f\"{key}.pt\")\n",
    "        if os.path.exists(out_path):\n",
    "            # Load, extend, save (simple, may be I/O heavy for huge files)\n",
    "            existing = torch.load(out_path, map_location='cpu', weights_only=False)\n",
    "            existing.extend(buf)\n",
    "            torch.save(existing, out_path)\n",
    "        else:\n",
    "            torch.save(buf, out_path)\n",
    "        buffers[key].clear()\n",
    "\n",
    "    print(f\"Processing {len(batch_files)} source batch files...\")\n",
    "    for bf in tqdm(batch_files, desc=\"Regrouping\"):\n",
    "        try:\n",
    "            data = torch.load(bf, map_location='cpu', weights_only=False)\n",
    "        except Exception as e:\n",
    "            print(f\"  Skipping {bf} (load error: {e})\")\n",
    "            continue\n",
    "\n",
    "        for rec in data:\n",
    "            sf = rec.get(\"source_file\", \"\")\n",
    "            # Drop trailing extension if present\n",
    "            sf_core = sf.rsplit('.', 1)[0]\n",
    "            parts = sf_core.split('_')\n",
    "            if len(parts) < prefix_parts:\n",
    "                key = sf_core  # fallback\n",
    "            else:\n",
    "                key = \"_\".join(parts[:prefix_parts])\n",
    "            buffers[key].append(rec)\n",
    "            counts[key] += 1\n",
    "\n",
    "            if flush_size and len(buffers[key]) >= flush_size:\n",
    "                flush_key(key)\n",
    "\n",
    "        # Free memory from loaded batch list\n",
    "        del data\n",
    "\n",
    "    # Flush remaining buffers\n",
    "    for key in list(buffers.keys()):\n",
    "        flush_key(key)\n",
    "\n",
    "    # Summary\n",
    "    print(\"\\nGrouping complete. Created:\")\n",
    "    for k in sorted(counts.keys()):\n",
    "        out_path = os.path.join(dst_dir, f\"{k}.pt\")\n",
    "        size_mb = os.path.getsize(out_path) / (1024*1024) if os.path.exists(out_path) else 0\n",
    "        print(f\"  {k}.pt  {counts[k]} records  ({size_mb:.1f} MB)\")\n",
    "\n",
    "    total = sum(counts.values())\n",
    "    print(f\"\\nTotal records regrouped: {total}\")\n",
    "\n",
    "# Run grouping\n",
    "regroup_batches_by_source_prefix(\n",
    "    src_dir=\"fen_moves_output_tensors\",\n",
    "    dst_dir=\"fen_moves_output_tensors_grouped\",\n",
    "    prefix_parts=2,\n",
    "    flush_size=50_000\n",
    ")\n",
    "# ...existing code..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed52d9e8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
